{
  "type": "doc-update",
  "created_at": "2026-01-31T01:35:50.483518+00:00",
  "status": "pending",
  "commit_message": "feat(heavy): add scale-first, context engineering, and recursive self-improvement constraints\n\nIMPLEMENTATION MODE now includes mandatory constraints:\n- Scale-first thinking: design for 10k+ entries/day, millions total\n- Risk-on, not risk-averse: prefer cutting-edge over incremental\n- Context engineering as core discipline: Four Buckets (WRITE/SELECT/COMPRESS/ISOLATE)\n- Recursive self-improvement: every session makes subsequent sessions smarter\n- Harness engineering integration: skills + hooks + memory = compounding loop\n- Technology defaults for scale: LanceDB, pgvector, HNSW, semantic search\n\nUpdates to agent prompts:\n- First Principles: now scales to 10k+/day, not just simplifies\n- AGI-Pilled: emphasizes recursive improvement and context engineering\n- Data Architect: includes tiered storage, vector indexing, migration paths\n\nNew anti-patterns to avoid:\n- Risk-averse thinking\n- Small-scale assumptions\n- Manual curation as a feature\n- File-based storage at scale\n- Token budget anxiety\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
  "diff_content": "commit b7afcf260192d13387b5cf3efdecb25b53811cce\nAuthor: olivier-motium <243932812+olivier-motium@users.noreply.github.com>\nDate:   Sat Jan 31 02:35:50 2026 +0100\n\n    feat(heavy): add scale-first, context engineering, and recursive self-improvement constraints\n    \n    IMPLEMENTATION MODE now includes mandatory constraints:\n    - Scale-first thinking: design for 10k+ entries/day, millions total\n    - Risk-on, not risk-averse: prefer cutting-edge over incremental\n    - Context engineering as core discipline: Four Buckets (WRITE/SELECT/COMPRESS/ISOLATE)\n    - Recursive self-improvement: every session makes subsequent sessions smarter\n    - Harness engineering integration: skills + hooks + memory = compounding loop\n    - Technology defaults for scale: LanceDB, pgvector, HNSW, semantic search\n    \n    Updates to agent prompts:\n    - First Principles: now scales to 10k+/day, not just simplifies\n    - AGI-Pilled: emphasizes recursive improvement and context engineering\n    - Data Architect: includes tiered storage, vector indexing, migration paths\n    \n    New anti-patterns to avoid:\n    - Risk-averse thinking\n    - Small-scale assumptions\n    - Manual curation as a feature\n    - File-based storage at scale\n    - Token budget anxiety\n    \n    Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>\n---\n config/skills/heavy/SKILL.md | 314 +++++++++++++++++++++++++++++++++++++------\n 1 file changed, 270 insertions(+), 44 deletions(-)\n\ndiff --git a/config/skills/heavy/SKILL.md b/config/skills/heavy/SKILL.md\nindex 113cdd4..4c7366b 100644\n--- a/config/skills/heavy/SKILL.md\n+++ b/config/skills/heavy/SKILL.md\n@@ -43,6 +43,136 @@ This user:\n **WRONG agent to spawn:** \"Let's debate whether model is actually smarter than human\"\n **RIGHT agent to spawn:** \"Given model > human assumption, how do we structure prompts to maximize reasoning?\"\n \n+---\n+\n+### IMPLEMENTATION MODE CONSTRAINTS (CRITICAL)\n+\n+**All agents in implementation mode MUST operate under these constraints:**\n+\n+#### 1. Scale-First Thinking (MANDATORY)\n+\n+**Assume massive scale by default.** Do not optimize for today's small state. Design for:\n+\n+| Metric | Assume This Scale |\n+|--------|-------------------|\n+| Data volume | 10,000+ entries/day, millions total |\n+| Retrieval latency | Sub-100ms P99 required |\n+| Human curation | Zero \u2014 the model decides everything |\n+| Context window | 200k+ tokens \u2014 use them, don't fear them |\n+\n+**WRONG**: \"MEMORIES.md works at 31 lines, keep it simple\"\n+**RIGHT**: \"At 10k lines/day, grep breaks. Design for vector search + hierarchical tiers from day 1.\"\n+\n+**Anti-patterns to reject:**\n+- \"This works for now\" \u2014 if it won't work at 1000x scale, don't propose it\n+- \"We can migrate later\" \u2014 migrations are expensive; build for scale now\n+- \"Manual curation as a feature\" \u2014 humans don't scale, models do\n+\n+#### 2. Risk-On, Not Risk-Averse\n+\n+**Be ambitious, not conservative.** The goal is cutting-edge architecture, not incremental fixes.\n+\n+| Risk-Averse (AVOID) | Risk-On (PREFER) |\n+|---------------------|------------------|\n+| \"Start with files, add database later\" | \"PostgreSQL + pgvector from day 1\" |\n+| \"Manual /compound trigger\" | \"Automatic capture on every session end\" |\n+| \"Grep over YAML frontmatter\" | \"HNSW vector index with hybrid search\" |\n+| \"Token budget anxiety (MAX_CHARS=2000)\" | \"Inject everything relevant, trust the model\" |\n+| \"Human reviews before storage\" | \"Model captures, model curates, model forgets\" |\n+\n+**The bar**: Would a god-tier AI SWE at a frontier lab propose this? If not, think bigger.\n+\n+#### 3. Context Engineering as Core Discipline\n+\n+**Every design decision is a context engineering decision.** Apply the Four Buckets Framework:\n+\n+| Bucket | Question to Ask | Example Application |\n+|--------|-----------------|---------------------|\n+| **WRITE** | What should persist outside context? | Memories \u2192 vector DB, not markdown files |\n+| **SELECT** | How do we retrieve only what's relevant? | Semantic search + metadata filters, not grep |\n+| **COMPRESS** | What can be summarized without loss? | Session \u2192 key learnings, not full transcript |\n+| **ISOLATE** | What deserves its own context window? | Each agent gets focused context, not monolithic dump |\n+\n+**Context engineering > prompt engineering.** The model is smart enough; the challenge is delivering the right information at the right time.\n+\n+#### 4. Recursive Self-Improvement (The Meta-Goal)\n+\n+**Every session should make subsequent sessions smarter.** This is the compounding loop:\n+\n+```\n+Session N executes task\n+    \u2192 Captures what worked and what didn't\n+    \u2192 Stores in searchable knowledge base\n+    \u2192 Session N+1 starts\n+    \u2192 Retrieves relevant learnings from N\n+    \u2192 Avoids N's mistakes, builds on N's successes\n+    \u2192 Captures new learnings\n+    \u2192 Session N+2 benefits from N and N+1\n+    \u2192 The system recursively improves\n+```\n+\n+**Components required for recursive improvement:**\n+\n+| Component | Purpose | Implementation |\n+|-----------|---------|----------------|\n+| **Automatic capture** | No human trigger, model decides what to remember | PostToolUse/Stop hooks with LLM extraction |\n+| **Semantic retrieval** | Find relevant memories without exact keywords | Vector embeddings + HNSW index |\n+| **Utility-based decay** | Forget unused memories, reinforce useful ones | Access tracking + decay scoring |\n+| **Cross-referencing** | New knowledge updates old knowledge | Zettelkasten-style bidirectional links |\n+| **Self-curation** | The memory system improves itself | Curator daemon with periodic consolidation |\n+\n+**The 13-commit test**: If the same edge case could be discovered 13 times across sessions (like the auto-approval saga in this repo), the system has failed. Recursive improvement means session #4 already knows what sessions #1-3 learned.\n+\n+#### 5. Harness Engineering Integration\n+\n+**The /heavy skill is part of a larger harness that enables autonomous improvement.**\n+\n+The harness architecture:\n+```\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  HARNESS: Skills + Hooks + Memory = Recursive Improvement          \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502                                                                     \u2502\n+\u2502  SKILLS (what to do)           HOOKS (when to do it)               \u2502\n+\u2502  \u251c\u2500 /heavy (analysis)          \u251c\u2500 SessionStart (inject context)   \u2502\n+\u2502  \u251c\u2500 /build (execution)         \u251c\u2500 PostToolUse (capture learnings) \u2502\n+\u2502  \u251c\u2500 /compound (capture)        \u251c\u2500 Stop (validate + archive)       \u2502\n+\u2502  \u2514\u2500 /remember (retrieve)       \u2514\u2500 Async (background curation)     \u2502\n+\u2502                                                                     \u2502\n+\u2502  MEMORY (what was learned)                                         \u2502\n+\u2502  \u251c\u2500 Hot tier: Recent solutions, always injected                   \u2502\n+\u2502  \u251c\u2500 Warm tier: Searchable archive, retrieved on demand            \u2502\n+\u2502  \u251c\u2500 Cold tier: Compressed patterns, long-term knowledge           \u2502\n+\u2502  \u2514\u2500 Graph: Cross-references between memories                       \u2502\n+\u2502                                                                     \u2502\n+\u2502  THE LOOP:                                                          \u2502\n+\u2502  Execute \u2192 Capture \u2192 Store \u2192 Retrieve \u2192 Execute (improved)        \u2502\n+\u2502                                                                     \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n+```\n+\n+**When designing in implementation mode:**\n+- Consider how the proposed system enables the compounding loop\n+- Identify where async hooks can capture knowledge automatically\n+- Design for the model to curate, not the human\n+- Ensure every session leaves the system smarter than before\n+\n+#### 6. Technology Defaults for Scale\n+\n+**When agents recommend infrastructure, prefer these defaults:**\n+\n+| Category | Default Choice | Why |\n+|----------|---------------|-----|\n+| **Vector store** | LanceDB (serverless) or PostgreSQL + pgvector | S3-backed, no server process, hy\n\n... [truncated - diff too long]",
  "existing_docs": [
    "docs/architecture.md",
    "docs/analysis-persistent-memory-for-harnesses.md",
    "docs/index.md",
    "docs/memory-integration-analysis.md",
    "docs/philosophy.md",
    "docs/audiobook-the-amnesiac-architect.md",
    "README.md",
    ".claude/MEMORIES.md"
  ],
  "instructions": "\nDOCUMENTATION UPDATE TASK\n\nA commit was just made in an repair/build session. Your job is to:\n\n1. Analyze the diff to understand what changed\n2. Determine which documentation files need updating\n3. Update the relevant docs to reflect the changes\n\nCOMMIT MESSAGE:\nfeat(heavy): add scale-first, context engineering, and recursive self-improvement constraints\n\nIMPLEMENTATION MODE now includes mandatory constraints:\n- Scale-first thinking: design for 10k+ entries/day, millions total\n- Risk-on, not risk-averse: prefer cutting-edge over incremental\n- Context engineering as core discipline: Four Buckets (WRITE/SELECT/COMPRESS/ISOLATE)\n- Recursive self-improvement: every session makes subsequent sessions smarter\n- Harness engineering integration: skills + hooks + memory = compounding loop\n- Technology defaults for scale: LanceDB, pgvector, HNSW, semantic search\n\nUpdates to agent prompts:\n- First Principles: now scales to 10k+/day, not just simplifies\n- AGI-Pilled: emphasizes recursive improvement and context engineering\n- Data Architect: includes tiered storage, vector indexing, migration paths\n\nNew anti-patterns to avoid:\n- Risk-averse thinking\n- Small-scale assumptions\n- Manual curation as a feature\n- File-based storage at scale\n- Token budget anxiety\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>\n\nEXISTING DOCUMENTATION FILES:\n- docs/architecture.md\n- docs/analysis-persistent-memory-for-harnesses.md\n- docs/index.md\n- docs/memory-integration-analysis.md\n- docs/philosophy.md\n- docs/audiobook-the-amnesiac-architect.md\n- README.md\n- .claude/MEMORIES.md\n\nDIFF CONTENT:\ncommit b7afcf260192d13387b5cf3efdecb25b53811cce\nAuthor: olivier-motium <243932812+olivier-motium@users.noreply.github.com>\nDate:   Sat Jan 31 02:35:50 2026 +0100\n\n    feat(heavy): add scale-first, context engineering, and recursive self-improvement constraints\n    \n    IMPLEMENTATION MODE now includes mandatory constraints:\n    - Scale-first thinking: design for 10k+ entries/day, millions total\n    - Risk-on, not risk-averse: prefer cutting-edge over incremental\n    - Context engineering as core discipline: Four Buckets (WRITE/SELECT/COMPRESS/ISOLATE)\n    - Recursive self-improvement: every session makes subsequent sessions smarter\n    - Harness engineering integration: skills + hooks + memory = compounding loop\n    - Technology defaults for scale: LanceDB, pgvector, HNSW, semantic search\n    \n    Updates to agent prompts:\n    - First Principles: now scales to 10k+/day, not just simplifies\n    - AGI-Pilled: emphasizes recursive improvement and context engineering\n    - Data Architect: includes tiered storage, vector indexing, migration paths\n    \n    New anti-patterns to avoid:\n    - Risk-averse thinking\n    - Small-scale assumptions\n    - Manual curation as a feature\n    - File-based storage at scale\n    - Token budget anxiety\n    \n    Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>\n---\n config/skills/heavy/SKILL.md | 314 +++++++++++++++++++++++++++++++++++++------\n 1 file changed, 270 insertions(+), 44 deletions(-)\n\ndiff --git a/config/skills/heavy/SKILL.md b/config/skills/heavy/SKILL.md\nindex 113cdd4..4c7366b 100644\n--- a/config/skills/heavy/SKILL.md\n+++ b/config/skills/heavy/SKILL.md\n@@ -43,6 +43,136 @@ This user:\n **WRONG agent to spawn:** \"Let's debate whether model is actually smarter than human\"\n **RIGHT agent to spawn:** \"Given model > human assumption, how do we structure prompts to maximize reasoning?\"\n \n+---\n+\n+### IMPLEMENTATION MODE CONSTRAINTS (CRITICAL)\n+\n+**All agents in implementation mode MUST operate under these constraints:**\n+\n+#### 1. Scale-First Thinking (MANDATORY)\n+\n+**Assume massive scale by default.** Do not optimize for today's small state. Design for:\n+\n+| Metric | Assume This Scale |\n+|--------|-------------------|\n+| Data volume | 10,000+ entries/day, millions total |\n+| Retrieval latency | Sub-100ms P99 required |\n+| Human curation | Zero \u2014 the model decides everything |\n+| Context window | 200k+ tokens \u2014 use them, don't fear them |\n+\n+**WRONG**: \"MEMORIES.md works at 31 lines, keep it simple\"\n+**RIGHT**: \"At 10k lines/day, grep breaks. Design for vector search + hierarchical tiers from day 1.\"\n+\n+**Anti-patterns to reject:**\n+- \"This works for now\" \u2014 if it won't work at 1000x scale, don't propose it\n+- \"We can migrate later\" \u2014 migrations are expensive; build for scale now\n+- \"Manual curation as a feature\" \u2014 humans don't scale, models do\n+\n+#### 2. Risk-On, Not Risk-Averse\n+\n+**Be ambitious, not conservative.** The goal is cutting-edge architecture, not incremental fixes.\n+\n+| Risk-Averse (AVOID) | Risk-On (PREFER) |\n+|---------------------|------------------|\n+| \"Start with files, add database later\" | \"PostgreSQL + pgvector from day 1\" |\n+| \"Manual /compound trigger\" | \"Automatic capture on every session end\" |\n+| \"Grep over YAML frontmatter\" | \"HNSW vector index with hybrid search\" |\n+| \"Token budget anxiety (MAX_CHARS=2000)\" | \"Inject everything relevant, trust the model\" |\n+| \"Human reviews before storage\" | \"Model captures, model curates, model forgets\" |\n+\n+**The bar**: Would a god-tier AI SWE at a frontier lab propose this? If not, think bigger.\n+\n+#### 3. Context Engineering as Core Discipline\n+\n+**Every design decision is a context engineering decision.** Apply the Four Buckets Framework:\n+\n+| Bucket | Question to Ask | Example Application |\n+|--------|-----------------|---------------------|\n+| **WRITE** | What should persist outside context? | Memories \u2192 vector DB, not markdown files |\n+| **SELECT** | How do we retrieve only what's relevant? | Semantic search + metadata filters, not grep |\n+| **COMPRESS** | What can be summarized without loss? | Session \u2192 key learnings, not full transcript |\n+| **ISOLATE** | What deserves its own context window? | Each agent gets focused context, not monolithic dump |\n+\n+**Context engineering > prompt engineering.** The model is smart enough; the challenge is delivering the right information at the right time.\n+\n+#### 4. Recursive Self-Improvement (The Meta-Goal)\n+\n+**Every session should make subsequent sessions smarter.** This is the compounding loop:\n+\n+```\n+Session N executes task\n+    \u2192 Captures what worked and what didn't\n+    \u2192 Stores in searchable knowledge base\n+    \u2192 Session N+1 starts\n+    \u2192 Retrieves relevant learnings from N\n+    \u2192 Avoids N's mistakes, builds on N's successes\n+    \u2192 Captures new learnings\n+    \u2192 Session N+2 benefits from N and N+1\n+    \u2192 The system recursively improves\n+```\n+\n+**Components required for recursive improvement:**\n+\n+| Component | Purpose | Implementation |\n+|-----------|---------|----------------|\n+| **Automatic capture** | No human trigger, model decides what to remember | PostToolUse/Stop hooks with LLM extraction |\n+| **Semantic retrieval** | Find relevant memories without exact keywords | Vector embeddings + HNSW index |\n+| **Utility-based decay** | Forget unused memories, reinforce useful ones | Access tracking + decay scoring |\n+| **Cross-referencing** | New knowledge updates old knowledge | Zettelkasten-style bidirectional links |\n+| **Self-curation** | The memory system improves itself | Curator daemon with periodic consolidation |\n+\n+**The 13-commit test**: If the same edge case could be discovered 13 times across sessions (like the auto-approval saga in this repo), the system has failed. Recursive improvement means session #4 already knows what sessions #1-3 learned.\n+\n+#### 5. Harness Engineering Integration\n+\n+**The /heavy skill is part of a larger harness that enables autonomous improvement.**\n+\n+The harness architecture:\n+```\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  HARNESS: Skills + Hooks + Memory = Recursive Improvement          \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502                                                                     \u2502\n+\u2502  SKILLS (what to do)           HOOKS (when to do it)               \u2502\n+\u2502  \u251c\u2500 /heavy (analysis)          \u251c\u2500 SessionStart (inject context)   \u2502\n+\u2502  \u251c\u2500 /build (execution)         \u251c\u2500 PostToolUse (capture learnings) \u2502\n+\u2502  \u251c\u2500 /compound (capture)        \u251c\u2500 Stop (validate + archive)       \u2502\n+\u2502  \u2514\u2500 /remember (retrieve)       \u2514\u2500 Async (background curation)     \u2502\n+\u2502                                                                     \u2502\n+\u2502  MEMORY (what was learned)                                         \u2502\n+\u2502  \u251c\u2500 Hot tier: Recent solutions, always injected                   \u2502\n+\u2502  \u251c\u2500 Warm tier: Searchable archive, retrieved on demand            \u2502\n+\u2502  \u251c\u2500 Cold tier: Compressed patterns, long-term knowledge           \u2502\n+\u2502  \u2514\u2500 Graph: Cross-references between memories                       \u2502\n+\u2502                                                                     \u2502\n+\u2502  THE LOOP:                                                          \u2502\n+\u2502  Execute \u2192 Capture \u2192 Store \u2192 Retrieve \u2192 Execute (improved)        \u2502\n+\u2502                                                                     \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n+```\n+\n+**When designing in implementation mode:**\n+- Consider how the proposed system enables the compounding loop\n+- Identify where async hooks can capture knowledge automatically\n+- Design for the model to curate, not the human\n+- Ensure every session leaves the system smarter than before\n+\n+#### 6. Technology Defaults for Scale\n+\n+**When agents recommend infrastructure, prefer these defaults:**\n+\n+| Category | Default Choice | Why |\n+|----------|---------------|-----|\n+| **Vector store** | LanceDB (serverless) or PostgreSQL + pgvector | S3-backed, no server process, hy\n\n... [truncated - diff too long]\n\nINSTRUCTIONS:\n- Only update docs that are actually affected by this change\n- Keep updates concise and accurate\n- Don't add unnecessary documentation\n- If the change is purely code (no architectural/API changes), you may skip doc updates\n- Focus on: API changes, new features, architectural decisions, configuration changes\n\nUse the /heavy skill if you need multiple perspectives on what to document.\n"
}