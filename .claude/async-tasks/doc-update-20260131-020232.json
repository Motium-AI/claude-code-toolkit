{
  "type": "doc-update",
  "created_at": "2026-01-31T02:02:32.470296+00:00",
  "status": "pending",
  "commit_message": "feat(compound): implement append-only memory event store\n\nReplace the rigid 11/16/10 categorical schema (which produced zero\noutput files) with a working compound memory system:\n\n- Add _memory.py: atomic JSON writes (F_FULLFSYNC + os.replace),\n  manifest-based fast reads, 90-day TTL + 500 event cap\n- Rewrite compound-context-loader: reads from ~/.claude/memory/,\n  scores events by recency (60%) + entity overlap (40%)\n- Add auto-capture to stop-validator: archives checkpoint as memory\n  event on every successful stop (zero user effort)\n- Simplify /compound SKILL.md from 241 to 95 lines\n- Delete solution-schema.md and docs/solutions/ (empty infrastructure)\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>",
  "diff_content": "commit 012f24f1e65616bf1b4a1e654a09678cb6bf2b03\nAuthor: olivier-motium <243932812+olivier-motium@users.noreply.github.com>\nDate:   Sat Jan 31 03:02:32 2026 +0100\n\n    feat(compound): implement append-only memory event store\n    \n    Replace the rigid 11/16/10 categorical schema (which produced zero\n    output files) with a working compound memory system:\n    \n    - Add _memory.py: atomic JSON writes (F_FULLFSYNC + os.replace),\n      manifest-based fast reads, 90-day TTL + 500 event cap\n    - Rewrite compound-context-loader: reads from ~/.claude/memory/,\n      scores events by recency (60%) + entity overlap (40%)\n    - Add auto-capture to stop-validator: archives checkpoint as memory\n      event on every successful stop (zero user effort)\n    - Simplify /compound SKILL.md from 241 to 95 lines\n    - Delete solution-schema.md and docs/solutions/ (empty infrastructure)\n    \n    Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>\n---\n config/hooks/_memory.py                            | 333 +++++++++++++++++++++\n config/hooks/compound-context-loader.py            | 158 ++++++----\n config/hooks/stop-validator.py                     |  81 ++++-\n config/skills/compound/SKILL.md                    | 258 ++++------------\n .../skills/compound/references/solution-schema.md  | 114 -------\n docs/solutions/.gitkeep                            |  44 ---\n 6 files changed, 564 insertions(+), 424 deletions(-)\n\ndiff --git a/config/hooks/_memory.py b/config/hooks/_memory.py\nnew file mode 100644\nindex 0000000..6b2576e\n--- /dev/null\n+++ b/config/hooks/_memory.py\n@@ -0,0 +1,333 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Memory primitives for Claude Code hooks.\n+\n+Append-only event store with crash-safe writes, manifest-based fast reads,\n+and project-scoped isolation via git remote hash.\n+\n+Used by:\n+- compound-context-loader.py (SessionStart: inject recent events)\n+- stop-validator.py (Stop: auto-capture checkpoint as event)\n+- /compound skill (manual: deep capture of solved problems)\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import fcntl\n+import hashlib\n+import json\n+import os\n+import subprocess\n+import tempfile\n+import time\n+from datetime import datetime, timezone\n+from pathlib import Path\n+from uuid import uuid4\n+\n+from _common import log_debug\n+\n+# ============================================================================\n+# Constants\n+# ============================================================================\n+\n+MEMORY_ROOT = Path.home() / \".claude\" / \"memory\"\n+EVENT_TTL_DAYS = 90\n+MAX_EVENTS = 500\n+MANIFEST_NAME = \"manifest.json\"\n+\n+\n+# ============================================================================\n+# Atomic Write (P0 crash safety)\n+# ============================================================================\n+\n+\n+def atomic_write_json(path: Path, data: dict) -> None:\n+    \"\"\"Write JSON atomically using write-temp-fsync-rename pattern.\n+\n+    Guarantees: the file at `path` is either the old content or the\n+    new content, never a partial write. Uses F_FULLFSYNC on macOS\n+    for true durability.\n+    \"\"\"\n+    path = Path(path)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+\n+    fd, tmp_path = tempfile.mkstemp(\n+        dir=str(path.parent),\n+        prefix=f\".{path.stem}.\",\n+        suffix=\".tmp\",\n+    )\n+    try:\n+        with os.fdopen(fd, \"w\") as f:\n+            json.dump(data, f, indent=2)\n+            f.write(\"\\n\")\n+            f.flush()\n+            # macOS fsync() doesn't flush disk write cache; F_FULLFSYNC does\n+            if hasattr(fcntl, \"F_FULLFSYNC\"):\n+                fcntl.fcntl(f.fileno(), fcntl.F_FULLFSYNC)\n+            else:\n+                os.fsync(f.fileno())\n+        os.replace(tmp_path, str(path))\n+    except BaseException:\n+        try:\n+            os.unlink(tmp_path)\n+        except OSError:\n+            pass\n+        raise\n+\n+\n+# ============================================================================\n+# Safe Read\n+# ============================================================================\n+\n+\n+def safe_read_event(path: Path) -> dict | None:\n+    \"\"\"Read a JSON event file with corruption detection.\n+\n+    Returns None for corrupt/empty files. Does not quarantine \u2014\n+    corrupt files are rare and cleanup handles them.\n+    \"\"\"\n+    try:\n+        raw = path.read_text(encoding=\"utf-8\")\n+        if not raw.strip():\n+            return None\n+        event = json.loads(raw)\n+        if not isinstance(event, dict):\n+            return None\n+        return event\n+    except (json.JSONDecodeError, IOError, OSError):\n+        return None\n+\n+\n+# ============================================================================\n+# Project Identity\n+# ============================================================================\n+\n+\n+def get_project_hash(cwd: str) -> str:\n+    \"\"\"Generate a stable, collision-resistant project identifier.\n+\n+    Uses SHA256(git_remote_url | repo_root)[:16]. Two repos on the\n+    same machine always get different hashes.\n+    \"\"\"\n+    try:\n+        remote = subprocess.run(\n+            [\"git\", \"remote\", \"get-url\", \"origin\"],\n+            capture_output=True, text=True, timeout=5, cwd=cwd or None,\n+        )\n+        remote_url = remote.stdout.strip()\n+\n+        root = subprocess.run(\n+            [\"git\", \"rev-parse\", \"--show-toplevel\"],\n+            capture_output=True, text=True, timeout=5, cwd=cwd or None,\n+        )\n+        repo_root = root.stdout.strip()\n+\n+        identity = f\"{remote_url}|{repo_root}\"\n+        return hashlib.sha256(identity.encode()).hexdigest()[:16]\n+    except (subprocess.TimeoutExpired, FileNotFoundError, OSError):\n+        # Fallback: hash the absolute path\n+        return hashlib.sha256(\n+            str(Path(cwd).resolve()).encode()\n+        ).hexdigest()[:16]\n+\n+\n+def get_memory_dir(cwd: str) -> Path:\n+    \"\"\"Get the memory directory for a project, creating it if needed.\"\"\"\n+    project_hash = get_project_hash(cwd)\n+    memory_dir = MEMORY_ROOT / project_hash / \"events\"\n+    memory_dir.mkdir(parents=True, exist_ok=True)\n+    return memory_dir\n+\n+\n+# ============================================================================\n+# Event Operations\n+# ============================================================================\n+\n+\n+def append_event(\n+    cwd: str,\n+    content: str,\n+    entities: list[str],\n+    event_type: str = \"compound\",\n+    source: str = \"compound\",\n+    meta: dict | None = None,\n+) -> Path:\n+    \"\"\"Append a new event to the store. Returns the event file path.\n+\n+    Filename includes timestamp + PID + random suffix for uniqueness\n+    without locking.\n+    \"\"\"\n+    event_dir = get_memory_dir(cwd)\n+    now = datetime.now(timezone.utc)\n+    ts = now.strftime(\"%Y%m%dT%H%M%S\")\n+    suffix = uuid4().hex[:6]\n+    event_id = f\"evt_{ts}-{os.getpid()}-{suffix}\"\n+\n+    event = {\n+        \"id\": event_id,\n+        \"ts\": now.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n+        \"type\": event_type,\n+        \"content\": content,\n+        \"entities\": entities,\n+        \"source\": source,\n+        \"meta\": meta or {},\n+    }\n+\n+    event_path = event_dir / f\"{event_id}.json\"\n+    atomic_write_json(event_path, event)\n+\n+    # Update manifest\n+    _update_manifest(event_dir, event_id)\n+\n+    log_debug(\n+        f\"Event appended: {event_id}\",\n+        hook_name=\"memory\",\n+        parsed_data={\"type\": event_type, \"entities\": entities[:5]},\n+    )\n+\n+    return event_path\n+\n+\n+def _update_manifest(event_dir: Path, new_event_id: str) -> None:\n+    \"\"\"Update manifest with new event ID. Best-effort, non-blocking.\"\"\"\n+    manifest_path = event_dir.parent / MANIFEST_NAME\n+    try:\n+        manifest = {}\n+        if manifest_path.exists():\n+            raw = manifest_path.read_text()\n+            if raw.strip():\n+                manifest = json.loads(raw)\n+\n+        recent = manifest.get(\"recent\", [])\n+        recent.insert(0, new_event_id)\n+        recent = recent[:50]  # Keep top 50\n+\n+        manifest[\"recent\"] = recent\n+        manifest[\"total_count\"] = manifest.get(\"total_count\", 0) +\n\n... [truncated - diff too long]",
  "existing_docs": [
    "docs/architecture.md",
    "docs/analysis-persistent-memory-for-harnesses.md",
    "docs/index.md",
    "docs/memory-integration-analysis.md",
    "docs/philosophy.md",
    "docs/audiobook-the-amnesiac-architect.md",
    "README.md",
    ".claude/MEMORIES.md"
  ],
  "instructions": "\nDOCUMENTATION UPDATE TASK\n\nA commit was just made in an repair/build session. Your job is to:\n\n1. Analyze the diff to understand what changed\n2. Determine which documentation files need updating\n3. Update the relevant docs to reflect the changes\n\nCOMMIT MESSAGE:\nfeat(compound): implement append-only memory event store\n\nReplace the rigid 11/16/10 categorical schema (which produced zero\noutput files) with a working compound memory system:\n\n- Add _memory.py: atomic JSON writes (F_FULLFSYNC + os.replace),\n  manifest-based fast reads, 90-day TTL + 500 event cap\n- Rewrite compound-context-loader: reads from ~/.claude/memory/,\n  scores events by recency (60%) + entity overlap (40%)\n- Add auto-capture to stop-validator: archives checkpoint as memory\n  event on every successful stop (zero user effort)\n- Simplify /compound SKILL.md from 241 to 95 lines\n- Delete solution-schema.md and docs/solutions/ (empty infrastructure)\n\nCo-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>\n\nEXISTING DOCUMENTATION FILES:\n- docs/architecture.md\n- docs/analysis-persistent-memory-for-harnesses.md\n- docs/index.md\n- docs/memory-integration-analysis.md\n- docs/philosophy.md\n- docs/audiobook-the-amnesiac-architect.md\n- README.md\n- .claude/MEMORIES.md\n\nDIFF CONTENT:\ncommit 012f24f1e65616bf1b4a1e654a09678cb6bf2b03\nAuthor: olivier-motium <243932812+olivier-motium@users.noreply.github.com>\nDate:   Sat Jan 31 03:02:32 2026 +0100\n\n    feat(compound): implement append-only memory event store\n    \n    Replace the rigid 11/16/10 categorical schema (which produced zero\n    output files) with a working compound memory system:\n    \n    - Add _memory.py: atomic JSON writes (F_FULLFSYNC + os.replace),\n      manifest-based fast reads, 90-day TTL + 500 event cap\n    - Rewrite compound-context-loader: reads from ~/.claude/memory/,\n      scores events by recency (60%) + entity overlap (40%)\n    - Add auto-capture to stop-validator: archives checkpoint as memory\n      event on every successful stop (zero user effort)\n    - Simplify /compound SKILL.md from 241 to 95 lines\n    - Delete solution-schema.md and docs/solutions/ (empty infrastructure)\n    \n    Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>\n---\n config/hooks/_memory.py                            | 333 +++++++++++++++++++++\n config/hooks/compound-context-loader.py            | 158 ++++++----\n config/hooks/stop-validator.py                     |  81 ++++-\n config/skills/compound/SKILL.md                    | 258 ++++------------\n .../skills/compound/references/solution-schema.md  | 114 -------\n docs/solutions/.gitkeep                            |  44 ---\n 6 files changed, 564 insertions(+), 424 deletions(-)\n\ndiff --git a/config/hooks/_memory.py b/config/hooks/_memory.py\nnew file mode 100644\nindex 0000000..6b2576e\n--- /dev/null\n+++ b/config/hooks/_memory.py\n@@ -0,0 +1,333 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Memory primitives for Claude Code hooks.\n+\n+Append-only event store with crash-safe writes, manifest-based fast reads,\n+and project-scoped isolation via git remote hash.\n+\n+Used by:\n+- compound-context-loader.py (SessionStart: inject recent events)\n+- stop-validator.py (Stop: auto-capture checkpoint as event)\n+- /compound skill (manual: deep capture of solved problems)\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import fcntl\n+import hashlib\n+import json\n+import os\n+import subprocess\n+import tempfile\n+import time\n+from datetime import datetime, timezone\n+from pathlib import Path\n+from uuid import uuid4\n+\n+from _common import log_debug\n+\n+# ============================================================================\n+# Constants\n+# ============================================================================\n+\n+MEMORY_ROOT = Path.home() / \".claude\" / \"memory\"\n+EVENT_TTL_DAYS = 90\n+MAX_EVENTS = 500\n+MANIFEST_NAME = \"manifest.json\"\n+\n+\n+# ============================================================================\n+# Atomic Write (P0 crash safety)\n+# ============================================================================\n+\n+\n+def atomic_write_json(path: Path, data: dict) -> None:\n+    \"\"\"Write JSON atomically using write-temp-fsync-rename pattern.\n+\n+    Guarantees: the file at `path` is either the old content or the\n+    new content, never a partial write. Uses F_FULLFSYNC on macOS\n+    for true durability.\n+    \"\"\"\n+    path = Path(path)\n+    path.parent.mkdir(parents=True, exist_ok=True)\n+\n+    fd, tmp_path = tempfile.mkstemp(\n+        dir=str(path.parent),\n+        prefix=f\".{path.stem}.\",\n+        suffix=\".tmp\",\n+    )\n+    try:\n+        with os.fdopen(fd, \"w\") as f:\n+            json.dump(data, f, indent=2)\n+            f.write(\"\\n\")\n+            f.flush()\n+            # macOS fsync() doesn't flush disk write cache; F_FULLFSYNC does\n+            if hasattr(fcntl, \"F_FULLFSYNC\"):\n+                fcntl.fcntl(f.fileno(), fcntl.F_FULLFSYNC)\n+            else:\n+                os.fsync(f.fileno())\n+        os.replace(tmp_path, str(path))\n+    except BaseException:\n+        try:\n+            os.unlink(tmp_path)\n+        except OSError:\n+            pass\n+        raise\n+\n+\n+# ============================================================================\n+# Safe Read\n+# ============================================================================\n+\n+\n+def safe_read_event(path: Path) -> dict | None:\n+    \"\"\"Read a JSON event file with corruption detection.\n+\n+    Returns None for corrupt/empty files. Does not quarantine \u2014\n+    corrupt files are rare and cleanup handles them.\n+    \"\"\"\n+    try:\n+        raw = path.read_text(encoding=\"utf-8\")\n+        if not raw.strip():\n+            return None\n+        event = json.loads(raw)\n+        if not isinstance(event, dict):\n+            return None\n+        return event\n+    except (json.JSONDecodeError, IOError, OSError):\n+        return None\n+\n+\n+# ============================================================================\n+# Project Identity\n+# ============================================================================\n+\n+\n+def get_project_hash(cwd: str) -> str:\n+    \"\"\"Generate a stable, collision-resistant project identifier.\n+\n+    Uses SHA256(git_remote_url | repo_root)[:16]. Two repos on the\n+    same machine always get different hashes.\n+    \"\"\"\n+    try:\n+        remote = subprocess.run(\n+            [\"git\", \"remote\", \"get-url\", \"origin\"],\n+            capture_output=True, text=True, timeout=5, cwd=cwd or None,\n+        )\n+        remote_url = remote.stdout.strip()\n+\n+        root = subprocess.run(\n+            [\"git\", \"rev-parse\", \"--show-toplevel\"],\n+            capture_output=True, text=True, timeout=5, cwd=cwd or None,\n+        )\n+        repo_root = root.stdout.strip()\n+\n+        identity = f\"{remote_url}|{repo_root}\"\n+        return hashlib.sha256(identity.encode()).hexdigest()[:16]\n+    except (subprocess.TimeoutExpired, FileNotFoundError, OSError):\n+        # Fallback: hash the absolute path\n+        return hashlib.sha256(\n+            str(Path(cwd).resolve()).encode()\n+        ).hexdigest()[:16]\n+\n+\n+def get_memory_dir(cwd: str) -> Path:\n+    \"\"\"Get the memory directory for a project, creating it if needed.\"\"\"\n+    project_hash = get_project_hash(cwd)\n+    memory_dir = MEMORY_ROOT / project_hash / \"events\"\n+    memory_dir.mkdir(parents=True, exist_ok=True)\n+    return memory_dir\n+\n+\n+# ============================================================================\n+# Event Operations\n+# ============================================================================\n+\n+\n+def append_event(\n+    cwd: str,\n+    content: str,\n+    entities: list[str],\n+    event_type: str = \"compound\",\n+    source: str = \"compound\",\n+    meta: dict | None = None,\n+) -> Path:\n+    \"\"\"Append a new event to the store. Returns the event file path.\n+\n+    Filename includes timestamp + PID + random suffix for uniqueness\n+    without locking.\n+    \"\"\"\n+    event_dir = get_memory_dir(cwd)\n+    now = datetime.now(timezone.utc)\n+    ts = now.strftime(\"%Y%m%dT%H%M%S\")\n+    suffix = uuid4().hex[:6]\n+    event_id = f\"evt_{ts}-{os.getpid()}-{suffix}\"\n+\n+    event = {\n+        \"id\": event_id,\n+        \"ts\": now.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n+        \"type\": event_type,\n+        \"content\": content,\n+        \"entities\": entities,\n+        \"source\": source,\n+        \"meta\": meta or {},\n+    }\n+\n+    event_path = event_dir / f\"{event_id}.json\"\n+    atomic_write_json(event_path, event)\n+\n+    # Update manifest\n+    _update_manifest(event_dir, event_id)\n+\n+    log_debug(\n+        f\"Event appended: {event_id}\",\n+        hook_name=\"memory\",\n+        parsed_data={\"type\": event_type, \"entities\": entities[:5]},\n+    )\n+\n+    return event_path\n+\n+\n+def _update_manifest(event_dir: Path, new_event_id: str) -> None:\n+    \"\"\"Update manifest with new event ID. Best-effort, non-blocking.\"\"\"\n+    manifest_path = event_dir.parent / MANIFEST_NAME\n+    try:\n+        manifest = {}\n+        if manifest_path.exists():\n+            raw = manifest_path.read_text()\n+            if raw.strip():\n+                manifest = json.loads(raw)\n+\n+        recent = manifest.get(\"recent\", [])\n+        recent.insert(0, new_event_id)\n+        recent = recent[:50]  # Keep top 50\n+\n+        manifest[\"recent\"] = recent\n+        manifest[\"total_count\"] = manifest.get(\"total_count\", 0) +\n\n... [truncated - diff too long]\n\nINSTRUCTIONS:\n- Only update docs that are actually affected by this change\n- Keep updates concise and accurate\n- Don't add unnecessary documentation\n- If the change is purely code (no architectural/API changes), you may skip doc updates\n- Focus on: API changes, new features, architectural decisions, configuration changes\n\nUse the /heavy skill if you need multiple perspectives on what to document.\n"
}